---
title: "Credit Card Defaults"
author: "Gergely Fenyvesi"
date: "4/19/2020"
output: pdf_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# I. Introduction
 In the following study I will try to predict credit card defaults using a public dataset available on the Kaggle site 
[here](https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset).
This dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005. The original dataset was first presented on the UCI Machine Learning Repository [here](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients).
 
 The dataset contains 25 variables:
 
 * ID: ID of each client
 * LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit
 * SEX: Gender (1=male, 2=female)
 * EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
 * MARRIAGE: Marital status (1=married, 2=single, 3=others)
 * AGE: Age in years
 * PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)
 * PAY_2: Repayment status in August, 2005 (scale same as above)
 * PAY_3: Repayment status in July, 2005 (scale same as above)
 * PAY_4: Repayment status in June, 2005 (scale same as above)
 * PAY_5: Repayment status in May, 2005 (scale same as above)
 * PAY_6: Repayment status in April, 2005 (scale same as above)
 * BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)
 * BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)
 * BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)
 * BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)
 * BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)
 * BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)
 * PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)
 * PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)
 * PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)
 * PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)
 * PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)
 * PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)
 * default.payment.next.month: Default payment (1=yes, 0=no)

 First I will partition the dataset into a training and test set, using 90% of the data for training purposes. We need to find the best algorithm to predict the default.payment.next.month value of the test set using the original or transformed values of the other columns. As the default is a binary event (1=yes, 0=no) the accuracy or the balanced accuracy can be used to evaluate our algorithm. We do not strictly need to use a loss function like the Residual Mean Squared Error (RMSE).
 
 \pagebreak

# II. Methods

First I explored the whole dataset to identify meaningful and meaningless columns in determining the default in next month. I worked with the whole dataset. Then using a 90-10 rule I partitioned the dataset into a training and a test set. I chose in an arbitrary manner 3 algorithms to work with:

 1. Decision / Regression tree (rpart): the choice was made to identify the 3 most meaningful columns from my pre-selection and use them later
 2. KNN: this is my personal favorite and I think that this algorithm is particularly useful in our case
 3. Randomforest (rf): this is an advanced version  of the regression tree, averaging regression tree values
 
To benchmark my selection, I executed first the 3 methods on all the columns of the dataset, then compared the obtained results with numbers obtained using the 3 most meaningful columns. With my trained models I created an ensemble. Finally to identify other models that can be added in the ensemble to further enhance results and benchmark again obtained numbers using these 3 models, I created a more general ensemble, using 8 models with the default parameters.

 \pagebreak

# III. Results

## 1. Exploring the dataset
```{r dataset_creation, message=FALSE, echo=FALSE, warning=FALSE}
###################################################################################
# Download dataset + explore + load basic libraries
###################################################################################
library(tidyverse)
library(lubridate)
library(caret)
library(readr)
url <- "https://raw.githubusercontent.com/FenyGeri/CreditCardDefaults/master/UCI_Credit_Card.csv"
data <- read_csv(url)

# data exploration
# number of reported cases
all_lines <- dim(data)[1]

# defaults
defaults <- data %>% filter(default.payment.next.month==1) %>% summarise(n=n()) %>% pull(n)

# % of defaults
def_perc <- defaults/all_lines

# number of clients = number of lines (one client cannot be reported twice as data concerns only 6 months)
cli <- data %>% select(ID) %>% n_distinct()

```

The Credit Card Defaults dataset (data) contains `r all_lines` client data from which `r defaults` clients defaulted in Sept 2005, resulting in a global default proportion of `r def_perc`. The number of distinct client IDs is `r cli` hence one client cannot be listed twice. The structure of the dataset is the following:

```{r dataset_visaulisation_1, message=FALSE, echo=FALSE, warning=FALSE}
str(data)
```

We can observe the summary statistics of each data column:

```{r dataset_visaulisation_2, message=FALSE, echo=FALSE, warning=FALSE}
summary(data)
```

I examined gender, age, education and payment information of the population, trying to find signals of default.

### A. Gender

Proportionally we have more defaults in the case of men, but this information need to be trreated with caution as female prevalence is higher in the dataset:
```{r gender, message=FALSE, echo=FALSE, warning=FALSE}
data %>% select(SEX) %>% 
  mutate(SEX = case_when(
    .$SEX == 1 ~ "Male",
    .$SEX == 2 ~ "Female")
  ) %>% table() %>% knitr::kable()

# in % / gender -> proportionally slightly more males are in default (to treat with care as we have mnore female observations)
data %>% select(SEX,default.payment.next.month) %>% 
  mutate(SEX = case_when(
    .$SEX == 1 ~ "Male",
    .$SEX == 2 ~ "Female")
  ) %>% 
  mutate(default.payment.next.month = case_when(
    .$default.payment.next.month == 0 ~ "No default",
    .$default.payment.next.month == 1 ~ "In default")
  ) %>% table() %>% prop.table(margin = 1) %>% knitr::kable()
```

The difference is not sigbificant enough to be qualified as a signal of more probable default.

### B. Age

Credit card debt is the most common between young, around 30 year old clients in our population data.

```{r age, message=FALSE, echo=FALSE, warning=FALSE, out.width="90%", out.height="90%"}
# younger generation with around 30y median age use credit cards mainly
data %>% ggplot(aes(AGE)) + geom_histogram()
```

```{r age_1, message=FALSE, echo=FALSE, warning=FALSE}
data %>% group_by(AGE) %>% summarise(n=n()) %>% arrange(desc(n)) %>% slice(1:10) %>% knitr::kable()
```

Basic statistics show a positive skew and a kurtosis close to normal. For information in the case of a standard normal distribution the mean equals 0, standard deviation is unit (1), skewness=0, kurtosis=3 amd mean=mode=median. For a positive skew we typically see that: mode < median < mean.

```{r age_2, message=FALSE, echo=FALSE, warning=FALSE}
# basic stats -> positive skew, typical: mode < median < mean, long distribution tail on the right
# for reference: standard normal distribution: mean=0, stdev=1, skew=0, kurtosis=3, mean=mode=median
library(moments)
#function to get the mode of a distribution
getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}
data %>% select(AGE) %>% summary() %>% knitr::kable()
data %>% summarise(AVG=mean(AGE), STDEV=sd(AGE), SKEW=skewness(AGE), KURTOSIS=kurtosis(AGE), MODE=getmode(AGE)) %>% knitr::kable()
```

When we try to visualise the default rate against the age of the clients we can see that the points are not randomly distributed around the mean. Defaults are under average around 30 and above average for younger and older generations.

```{r age_3, message=FALSE, echo=FALSE, warning=FALSE}
# seems however there is a connection between age and defaults
# defaults are under average around 30 and above average for younger and older generations
# the points are not randomly distributed around the average
# we should try to use age as a variable
data %>% group_by(AGE) %>% summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(percent=default/all) %>% ggplot(aes(AGE,percent)) + geom_point() + 
  geom_smooth() + geom_hline(yintercept = defaults/all_lines) + 
  geom_text(aes(20,defaults/all_lines,label="average default",vjust=1, hjust=0.3)) + 
  scale_y_continuous("Default")
```

If we create age groups this connection between age and default rates is even more pronounced. However it is to treat with caution as the different age groups have different number of participants. It is common to observe a regression to the mean with higher number of observations. 

```{r age_4, message=FALSE, echo=FALSE, warning=FALSE}
# create age intervals -> round to 10 y
data %>% mutate(AGE=as.integer(round(AGE/10))*10)  %>% group_by(AGE) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all) %>% filter(all>10) %>% arrange(proportion) %>% knitr::kable()

# we can see much better the tendancy
data %>% mutate(AGE=as.integer(round(AGE/10))*10)  %>% group_by(AGE) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(percent=default/all) %>% filter(all>10) %>% 
  ggplot(aes(AGE,percent)) + geom_point(size=4) + geom_hline(aes(yintercept = defaults/all_lines)) + geom_smooth() + 
  geom_text(aes(20,defaults/all_lines,label="average default",vjust=-1, hjust=-6)) + 
  scale_y_continuous("Default")
```

### C. Education

Most clients are graduates and undergraduates (1 and 2), and a smaller proportion earned a high school degree. Other education is marginal and can be excluded.

```{r edu, message=FALSE, echo=FALSE, warning=FALSE}
# mainly higher education people (graduate + university)
# the low number of observations in some groups does not allow general conclusions
data %>% select(EDUCATION) %>% table() %>% knitr::kable()
data %>% ggplot(aes(EDUCATION)) + geom_bar()
```

Defaults are lower for higher education clients. 

```{r edu_2, message=FALSE, echo=FALSE, warning=FALSE}
# defaults per group
# should retain only group 1-2-3 and conclude that higher education lowers default probability
# in other groups we do not have enough observations
data %>% group_by(EDUCATION) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all)  %>% filter(all>500) %>%
  arrange(proportion) %>% knitr::kable()
  
```

### D. Marital status

Single persons seems to make less frequently default.

```{r mar, message=FALSE, echo=FALSE, warning=FALSE}
#############################################################
# MARRIAGE: Marital status (1=married, 2=single, 3=others)
#############################################################
# lower for single, higher form married and other
data %>% group_by(MARRIAGE) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all)  %>% arrange(proportion) %>% knitr::kable()

```

 \pagebreak

### E. Repayment status in September, 2005 (PAY_0)

We can observe a positive relationship between late payments and defaults. The higher the number, the higher the delay of payment of credit card debt.

```{r pay, message=FALSE, echo=FALSE, warning=FALSE}
##########################################################################################################################
# PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 
#     8=payment delay for eight months, 9=payment delay for nine months and above)
##########################################################################################################################
# we can observe a clear relationship between delayed payments and defaults
data %>% group_by(PAY_0) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all)  %>% arrange(proportion)  %>% knitr::kable()

data %>% group_by(PAY_0) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(percent=default/all)  %>% ggplot(aes(PAY_0,percent)) + geom_point() + geom_smooth(method=lm) + 
  scale_y_continuous("Default")

```

 \pagebreak

### F. Amount of bill statement in September, 2005 (BILL_AMT1)

Most people have a low bill amount, but we cannot clearly see details on a simple linear x-axis.

```{r bill, message=FALSE, echo=FALSE, warning=FALSE}
data %>% ggplot(aes(BILL_AMT1)) + geom_histogram(bins=20) 
```

We could be tempted to use a log transformation of the x-axis, but in this case we will lose 0 values (log10 of 0 = -Inf). The correct way is to transorm 0 amounts to 1 before the log transformation.

```{r bill_1, message=FALSE, echo=FALSE, warning=FALSE}
# by transforming 0 amounts to 1 we can treat this issue
data %>%  mutate(BILL_AMT1 = case_when(
  .$BILL_AMT1 == 0 ~ 1,
  .$BILL_AMT1 != 0  ~ .$BILL_AMT1)
) %>% ggplot(aes(BILL_AMT1)) + geom_histogram(bins=20) + scale_x_log10()
```

To show whether there is a relationship between bill amount and default probability I created two groups, higher and lower then the median value and examined default probability. With only two groups we see no evidence.

```{r bill_2, message=FALSE, echo=FALSE, warning=FALSE}
data %>% select(BILL_AMT1, default.payment.next.month) %>%
  mutate(BILL_AMT1 = case_when(
    .$BILL_AMT1 >= median(data$BILL_AMT1) ~ "More",
    .$BILL_AMT1 < median(data$BILL_AMT1) ~ "Less")
  ) %>% table()  %>% prop.table(margin = 1) %>% knitr::kable()
```

By rounding bill amounts to the nearest 10k NT dollars I created more groups. We see some evidence of relationship.

```{r bill_3, message=FALSE, echo=FALSE, warning=FALSE}
data %>% mutate(BILL_AMT1=as.integer(round(BILL_AMT1/10000))) %>% 
  mutate(BILL_AMT1=as.integer(round(BILL_AMT1/10)*10)) %>% 
  group_by(BILL_AMT1) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all)  %>% arrange(desc(proportion)) %>% knitr::kable()
```

 \pagebreak

By excluding classes with too few observations then plotting them we can see that higher amount means higher risk of default.

```{r bill_4, message=FALSE, echo=FALSE, warning=FALSE}
data %>% mutate(BILL_AMT1=as.integer(round(BILL_AMT1/10000))) %>% 
  mutate(BILL_AMT1=as.integer(round(BILL_AMT1/10)*10)) %>% 
  group_by(BILL_AMT1) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all)  %>% 
  filter(all>10) %>% ggplot(aes(BILL_AMT1,proportion)) + geom_point() +
  geom_smooth(method = "lm")
```

### G. Amount of previous payment in September, 2005 (PAY_AMT1)

Instead of examining the payment amount I checked its proportion to the bill amount (BILL_AMT1/PAY_AMT1). I filtered out where PAY_AMT1 equals to 0, then compared the proportion for defaults and no defaults. For defaults this proportion was significantly higher (> 2x). This could be treated as a signal but with caution as the standard deviation is huge compared to the average values.

```{r paym, message=FALSE, echo=FALSE, warning=FALSE}
data %>% filter(PAY_AMT1!=0) %>%   
  mutate(prop=BILL_AMT1/PAY_AMT1) %>% group_by(default.payment.next.month) %>%
  summarise(avg=mean(prop), stdev=sd(prop)) %>% knitr::kable()
```

 \pagebreak

Visually we cannot clearly distinguish the two boxes as one includes the other.

```{r paym_2, message=FALSE, echo=FALSE, warning=FALSE}
data %>% filter(PAY_AMT1!=0) %>%   
  mutate(prop=BILL_AMT1/PAY_AMT1, Default=factor(default.payment.next.month)) %>% 
  ggplot(aes(Default,prop)) +
  geom_boxplot() + scale_y_continuous(trans="log10", "BILL_AMT1/PAY_AMT1") 
```

 \pagebreak

### H. Amount of given credit (LIMIT_BAL)

Again instead of checking the amount I checked the proportion to the bill amount (BILL_AMT1/LIMIT_BAL). Logically a bill amount closer to the limit means higher risk and higher default probability. In this case we don't need to filter out where LIMIT_BAL = 0 because simply it has no meaning. For defaults this proportion is a bit higher (+20%). This could be a signal, but it is to treat with caution as the standard deviation is significant compared to the average values.

```{r lim, message=FALSE, echo=FALSE, warning=FALSE}
data %>% 
  mutate(prop=BILL_AMT1/LIMIT_BAL) %>% group_by(default.payment.next.month) %>%
  summarise(avg=mean(prop), stdev=sd(prop)) %>% knitr::kable()
```

Visually again we cannot clearly distinguish the two boxes.

```{r lim_1, message=FALSE, echo=FALSE, warning=FALSE}
data %>% 
  mutate(prop=BILL_AMT1/LIMIT_BAL, Default=factor(default.payment.next.month)) %>% 
  ggplot(aes(Default,prop)) +
  geom_boxplot() + scale_y_continuous(trans="log2", "BILL_AMT1/LIMIT_BAL")
```

 \pagebreak

### I. Credit migration

I had the idea to check the deterioration of credit quality over time, by exmining the difference between Sept, 2005 and April, 2005 payments. When we check the payments for the first 5 users we see that it is not stable over time: it can change in the two directions.

```{r mig_1, message=FALSE, echo=FALSE, warning=FALSE}
data %>% filter(ID<=5) %>%
  select(ID, PAY_0, PAY_2, PAY_3, PAY_4, PAY_5, PAY_6) %>%
  gather(PAY,STATUS, PAY_0:PAY_6) %>% 
  mutate(MONTHS = case_when(
    .$PAY == "PAY_0" ~ -1,
    .$PAY == "PAY_2" ~ -2,
    .$PAY == "PAY_3" ~ -3,
    .$PAY == "PAY_4" ~ -4,
    .$PAY == "PAY_5" ~ -5,
    .$PAY == "PAY_6" ~ -6)
  ) %>% 
  ggplot(aes(MONTHS, STATUS, group=ID)) + geom_line() +
  facet_grid(. ~ ID)
```

The proper solution would be to fit a line for each user and use the slope as a sign to determine whether the financial status became better or worse. For simplicity I took the delta between PAY_6 and PAY_0 (MIGRATION = PAY_0-PAY_6). I filtered out classes with too few observations, but evidence were still weak.

```{r mig_2, message=FALSE, echo=FALSE, warning=FALSE}
data %>% mutate(MIGRATION = PAY_0-PAY_6) %>% group_by(MIGRATION) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all)  %>% 
  filter(all>50) %>% arrange(proportion) %>% knitr::kable()

data %>% mutate(MIGRATION = PAY_0-PAY_6) %>% group_by(MIGRATION) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(percent=default/all)  %>% 
  filter(all>50) %>% ggplot(aes(MIGRATION,percent)) + geom_point() + geom_smooth(method=lm) + 
  scale_y_continuous("Default")
```

Stable ratings (where MIGRATION is close to 0) seems to make less frequently default. I tried to apply abs as transformation then examnined and plotted results again.

```{r mig_3, message=FALSE, echo=FALSE, warning=FALSE}
data %>% mutate(MIGRATION = abs(PAY_0-PAY_6)) %>% group_by(MIGRATION) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(proportion=default/all)  %>% arrange(proportion) %>% knitr::kable()

data %>% mutate(MIGRATION = abs(PAY_0-PAY_6)) %>% group_by(MIGRATION) %>% 
  summarise(all=n(),default=sum(default.payment.next.month)) %>%
  mutate(percent=default/all)  %>% ggplot(aes(MIGRATION,percent)) + geom_point() + geom_smooth(method=lm) + 
  scale_y_continuous("Default")
```

The evidence was much stronger, the line fits better on the points. I created a transfomed column using the formula MIGRATION = abs(PAY_0-PAY_6).

 \pagebreak

## 2. Creation of training and test dataset

I partitioned the dataset into 2 groups randomly: train and test dataset, using 90% of the data for training purposes and 10% for testing. I used set seed to guarantee the same result at each execution.

```{r part, warning=FALSE}
################################
# division to train + test set
################################
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = data$default.payment.next.month, times = 1, p = 0.1, list = FALSE)
train <- data[-test_index,]
test <- data[test_index,]
```

 \pagebreak

## 3. Test of the 3 algorithms using all columns

To benchmark results I started by guessing the default, then testing the 3 algorithms using all the columns of the dataset except the customer id. To avoid automatic selection of only a subset of columns by the regression tree, I executed the rpart training once with default complexity parameter and a second time with cp forced to 0. This parameter sets a minimum for how much the RSS must improve for another partition to be added and 0 value gives the highest flexibility and the maximum number of branches created. 

```{r guess, message=FALSE, echo=FALSE, warning=FALSE}
train <- train %>% select(-ID)
test <- test %>% select(-ID)
train <- train %>% mutate(default.payment.next.month=factor(default.payment.next.month))
test <- test %>% mutate(default.payment.next.month=factor(default.payment.next.month))

y_hat <- factor(sample(0:1,3000,TRUE,c((all_lines-defaults)/all_lines,defaults/all_lines)))
results <- data_frame(method = "guess", 
                      Accuracy = confusionMatrix(y_hat, test$default.payment.next.month)$overall[["Accuracy"]],
                      Sensitivity = confusionMatrix(y_hat, test$default.payment.next.month)$byClass[["Sensitivity"]],
                      Specificity = confusionMatrix(y_hat, test$default.payment.next.month)$byClass[["Specificity"]],
                      Balanced_Accuracy = confusionMatrix(y_hat, test$default.payment.next.month)$byClass[["Balanced Accuracy"]],
                      )
```

```{r knn, message=FALSE, echo=FALSE, warning=FALSE}
##########################################################################
# test knn with all columns except customer id, using default parameters
##########################################################################

train_knn <- train(default.payment.next.month ~ ., method = "knn", data = train)
y_hat_knn <- predict(train_knn, test, type = "raw")
conf_knn <- confusionMatrix(y_hat_knn, test$default.payment.next.month)
#confusionMatrix(y_hat_knn, test$default.payment.next.month)$overall[["Accuracy"]]
#confusionMatrix(y_hat_knn, test$default.payment.next.month)$byClass[["Sensitivity"]]
#confusionMatrix(y_hat_knn, test$default.payment.next.month)$byClass[["Specificity"]]
#confusionMatrix(y_hat_knn, test$default.payment.next.month)$byClass[["Balanced Accuracy"]]
# accuracy is quite high, sensitivity is high , but specificity is poor
# good accuracy is obtained, because prevalence of no default (0) is high and we previewed well the no defaults
# but defaults (default.payment.next.month=1) were poorly previewed, we previewed no default in much more cases
# balanced accuracy is not high, we are not really doing better than guessing with p=0.2 the default
# k used by default = 9 that gave the best accuracy
plot_knn <- ggplot(train_knn, highlight = TRUE)
bestk <- train_knn$bestTune
# we keep results
results <- bind_rows(results,data_frame(method = "knn", 
                      Accuracy = confusionMatrix(y_hat_knn, test$default.payment.next.month)$overall[["Accuracy"]],
                      Sensitivity = confusionMatrix(y_hat_knn, test$default.payment.next.month)$byClass[["Sensitivity"]],
                      Specificity = confusionMatrix(y_hat_knn, test$default.payment.next.month)$byClass[["Specificity"]],
                      Balanced_Accuracy = confusionMatrix(y_hat_knn, test$default.payment.next.month)$byClass[["Balanced Accuracy"]],
                      ))

```

With the default parameters R tunes the number of k to `r bestk` using knn:

```{r knn_1,  message=FALSE, echo=FALSE, warning=FALSE}
plot_knn
```

```{r rpart_1,  message=FALSE, echo=FALSE, warning=FALSE}
######################################################################################
# test regression tree with all columns except customer id, using default parameters
######################################################################################
train_rpart <- train(default.payment.next.month ~ ., 
                     method = "rpart",
                     data = train)
plotrpart <- ggplot(train_rpart)
bestcp <- train_rpart$bestTune
#train_rpart$finalModel
y_hat_rpart <- predict(train_rpart, test, type = "raw")
# we have better results, but we still predict with a relatively low specificity, failing to predict default in 2/3 of the cases
# the autumatically produced regression tree used payment status of last and 3rd month
# as we could expect, the PAY_0 column had the most impact (the payment status of the last month)
#plot(train_rpart$finalModel, margin = 0.1)
#text(train_rpart$finalModel, cex = 0.75)
# we keep results
results <- bind_rows(results,data_frame(method = "rpart", 
                      Accuracy = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$overall[["Accuracy"]],
                      Sensitivity = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$byClass[["Sensitivity"]],
                      Specificity = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$byClass[["Specificity"]],
                      Balanced_Accuracy = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$byClass[["Balanced Accuracy"]],
))
```

 \pagebreak

The default cp used is `r bestcp`:

```{r rpart_2,  message=FALSE, echo=FALSE, warning=FALSE}
plotrpart
```

We can visaulize the regression tree to see which columns were used:

```{r rpart_3,  message=FALSE, echo=FALSE, warning=FALSE}
plot(train_rpart$finalModel, margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)
```


It is not surprising to see on first place the repayment status of the last month. The PAY_3 variable on the second place is more just the consequence of overtraining, it has no more explanatory power then the PAY_0 column. By setting the cp to 0 we can force the usage of more (possibly all) columns in the data.


```{r rpart_4,  message=FALSE, echo=FALSE, warning=FALSE}
# for max flexibility we can set cp=0
train_rpart <- train(default.payment.next.month ~ ., 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0, len = 1)),
                     data = train)
y_hat_rpart <- predict(train_rpart, test, type = "raw")
#confusionMatrix(y_hat_rpart, test$default.payment.next.month)
# we have lower accuracy, but we predict with a slightly better specificity, but making more errors predicting 0
# the regression tree becomes really complicated
# as we could expect, the PAY_0 column had still the most impact (the payment status of the last month)
plot(train_rpart$finalModel, margin = 0.1)
#text(train_rpart$finalModel, cex = 0.75)

# we keep results
results <- bind_rows(results,data_frame(method = "rpart cp=0", 
                                        Accuracy = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$overall[["Accuracy"]],
                                        Sensitivity = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$byClass[["Sensitivity"]],
                                        Specificity = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$byClass[["Specificity"]],
                                        Balanced_Accuracy = confusionMatrix(y_hat_rpart, test$default.payment.next.month)$byClass[["Balanced Accuracy"]],
))
```

The variable importance can change but PAY_0 is still the most important variable:

```{r rpart_5,  message=FALSE, echo=FALSE, warning=FALSE}
var_imp <- data_frame(var = train_rpart$finalModel$frame$var[[1]],
                      importance = train_rpart$finalModel$variable.importance[[1]]
                      )

for(i in 2:10){
  var_imp <- bind_rows(var_imp,data_frame(var = train_rpart$finalModel$frame$var[[i]],
                        importance = train_rpart$finalModel$variable.importance[[i]]
  ))
}

var_imp %>% knitr::kable()
```

The global results of the 3 algorithms are the following:

```{r rf,  message=FALSE, echo=FALSE, warning=FALSE}
######################################################################################
# randomforest with all columns except customer id, using default parameters
######################################################################################
library(randomForest)
set.seed(1, sample.kind="Rounding")
train_rf <- randomForest(default.payment.next.month ~ ., data=train)
y_hat_rf <- predict(train_rf, test)
#confusionMatrix(y_hat_rf,test$default.payment.next.month)
# we have even better results using random forest trees, specificity and balanced accuracy are both highest
results <- bind_rows(results,data_frame(method = "rforest", 
                                        Accuracy = confusionMatrix(y_hat_rf, test$default.payment.next.month)$overall[["Accuracy"]],
                                        Sensitivity = confusionMatrix(y_hat_rf, test$default.payment.next.month)$byClass[["Sensitivity"]],
                                        Specificity = confusionMatrix(y_hat_rf, test$default.payment.next.month)$byClass[["Specificity"]],
                                        Balanced_Accuracy = confusionMatrix(y_hat_rf, test$default.payment.next.month)$byClass[["Balanced Accuracy"]],
))

results %>% knitr::kable()
```

Accuracy is quite high, but we should not stop here. Specificity is especially poor in the case of knn. Other algorithms failed as well when we tried to predict defaults. The high accuracy is just the consequence of the high sensitivity. Prevalence of no default in our data is 4 times higher then defaults. 
 Our results are not much better then guessing using the global average default rate. The table part of the confusionmatrix shows that in the case of knn we almost missed predicting default in 5/6th of the cases:

```{r cfm,  message=FALSE, echo=FALSE, warning=FALSE}
conf_knn$table %>% knitr::kable()
```

 \pagebreak

## 4. Working with selected columns

### A. Choosing the 3 most important variables

I started working with a subset of columns and the regression tree to identify the 3 most important variables that I used later:

 * SEX
 * EDUCATION
 * MARRIAGE
 * AGE
 * PAY_0
 * PROP1 = BILL_AMT1/PAY_AMT1
 * PROP2 = BILL_AMT1/LIMIT_BAL
 * MIGRATION = abs(PAY_0-PAY_6)

I removed NA, Inf, -Inf obtained during the transformation of data.

```{r rp_sel,  message=FALSE, echo=FALSE, warning=FALSE}
# add new columns + select columns to use + remove NA, Inf, -Inf
train2 <- train %>% mutate(PROP1=BILL_AMT1/PAY_AMT1, PROP2=BILL_AMT1/LIMIT_BAL, MIGRATION = abs(PAY_0-PAY_6)) %>% 
  select(SEX,EDUCATION,MARRIAGE,AGE,PAY_0,PROP1,PROP2,MIGRATION,default.payment.next.month)
train2 <- rbind(train2 %>% filter(is.na(PROP1)) %>% mutate(PROP1=0),train2 %>% filter(!is.na(PROP1)))
train2 <- rbind(train2 %>% filter(PROP1==Inf) %>% mutate(PROP1=1000),train2 %>% filter(PROP1!=Inf))
train2 <- rbind(train2 %>% filter(PROP1==-Inf) %>% mutate(PROP1=-1000),train2 %>% filter(PROP1!=-Inf))

test2 <- test %>% mutate(PROP1=BILL_AMT1/PAY_AMT1, PROP2=BILL_AMT1/LIMIT_BAL, MIGRATION = abs(PAY_0-PAY_6)) %>% 
  select(SEX,EDUCATION,MARRIAGE,AGE,PAY_0,PROP1,PROP2,MIGRATION,default.payment.next.month)
test2 <- rbind(test2 %>% filter(is.na(PROP1)) %>% mutate(PROP1=0),test2 %>% filter(!is.na(PROP1)))
test2 <- rbind(test2 %>% filter(PROP1==Inf) %>% mutate(PROP1=1000),test2 %>% filter(PROP1!=Inf))
test2 <- rbind(test2 %>% filter(PROP1==-Inf) %>% mutate(PROP1=-1000),test2 %>% filter(PROP1!=-Inf))

# let's start with rpart to identify the top 3 most important indicators and use them
# rpart -> forcing R to use more columns with a cp close to 0
train_rpart <- train(default.payment.next.month ~ ., 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.001, len = 25)),
                     data = train2)
y_hat_rpart <- predict(train_rpart, test2, type = "raw")
confusionMatrix(y_hat_rpart, test2$default.payment.next.month)
```

Results are slightly better but we still predict with a relatively low specificity, failing to predict default in 2/3 of the cases. Forcing the cp close to 0 created a regression tree with PAY_0, MIGRATION, PROP2 and EDUCATION columns. As we could expect, the PAY_0 (the payment status of the last month) column had still the most impact:

```{r rp_sel_1,  message=FALSE, echo=FALSE, warning=FALSE}
plot(train_rpart$finalModel, margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)
```

For the final algorithm I retained only 3 variables: PAY_0, MIGRATION and PROP2. I tried to predict defaults using the latest payment status, its evolution during the observed period and the distance from the accorded limit by the bank for each client. Using less variables helped to reduce computing time as well. For bigger datasets it could be a real advantage.

### B. Results using only 3 columns

For the rpart algorithm I forced cp to be close to 0, by this way I guaranteed using all columns instead of making the decision tree only on PAY_0.

```{r fin_1,  message=FALSE, echo=FALSE, warning=FALSE}
# we still force cp close to 0 to use all the columns
train_rpart <- train(default.payment.next.month ~ PAY_0+PROP2+MIGRATION, 
                     method = "rpart",
                     tuneGrid = data.frame(cp = seq(0, 0.001, len = 25)),
                     data = train2)

y_hat_rpart <- predict(train_rpart, test2, type = "raw")
plot(train_rpart$finalModel, margin = 0.1)
text(train_rpart$finalModel, cex = 0.75)


# add results
results2 <- data_frame(method = "rpart", 
                      Accuracy = confusionMatrix(y_hat_rpart, test2$default.payment.next.month)$overall[["Accuracy"]],
                      Sensitivity = confusionMatrix(y_hat_rpart, test2$default.payment.next.month)$byClass[["Sensitivity"]],
                      Specificity = confusionMatrix(y_hat_rpart, test2$default.payment.next.month)$byClass[["Specificity"]],
                      Balanced_Accuracy = confusionMatrix(y_hat_rpart, test2$default.payment.next.month)$byClass[["Balanced Accuracy"]],
)


# knn
# we should try to train with different k values, using cross validation
control <- trainControl(method = "cv", number = 10, p = .9)
train_knn <- train(default.payment.next.month ~ PAY_0+MIGRATION+PROP2, 
                   method = "knn", 
                   tuneGrid = data.frame(k = seq(1,100,5)),
                   trControl = control,
                   data = train2)
plot_knn <- ggplot(train_knn, highlight = TRUE)
nr_k <- train_knn$bestTune
y_hat_knn <- predict(train_knn, test2, type = "raw")
results2 <- bind_rows(results2,data_frame(method = "knn", 
                      Accuracy = confusionMatrix(y_hat_knn, test2$default.payment.next.month)$overall[["Accuracy"]],
                      Sensitivity = confusionMatrix(y_hat_knn, test2$default.payment.next.month)$byClass[["Sensitivity"]],
                      Specificity = confusionMatrix(y_hat_knn, test2$default.payment.next.month)$byClass[["Specificity"]],
                      Balanced_Accuracy = confusionMatrix(y_hat_knn, test2$default.payment.next.month)$byClass[["Balanced Accuracy"]],
))

# randomforest
set.seed(1, sample.kind="Rounding")
train_rf <- randomForest(default.payment.next.month ~ PAY_0+MIGRATION+PROP2, data=train2)
y_hat_rf <- predict(train_rf, test2)
# we have even better results using random forest trees, specificity an d balanced accuracy are both highest
results2 <- bind_rows(results2,data_frame(method = "rforest", 
                                        Accuracy = confusionMatrix(y_hat_rf, test2$default.payment.next.month)$overall[["Accuracy"]],
                                        Sensitivity = confusionMatrix(y_hat_rf, test2$default.payment.next.month)$byClass[["Sensitivity"]],
                                        Specificity = confusionMatrix(y_hat_rf, test2$default.payment.next.month)$byClass[["Specificity"]],
                                        Balanced_Accuracy = confusionMatrix(y_hat_rf, test2$default.payment.next.month)$byClass[["Balanced Accuracy"]],
))


# let's create an ensemble with the 3 models
y_hat_ensemble <- factor(ifelse(round((as.numeric(y_hat_knn) + as.numeric(y_hat_rpart) + as.numeric(y_hat_rf))/3)==1,0,1))
results2 <- bind_rows(results2,data_frame(method = "ensemble", 
                                          Accuracy = confusionMatrix(y_hat_ensemble, test2$default.payment.next.month)$overall[["Accuracy"]],
                                          Sensitivity = confusionMatrix(y_hat_ensemble, test2$default.payment.next.month)$byClass[["Sensitivity"]],
                                          Specificity = confusionMatrix(y_hat_ensemble, test2$default.payment.next.month)$byClass[["Specificity"]],
                                          Balanced_Accuracy = confusionMatrix(y_hat_ensemble, test2$default.payment.next.month)$byClass[["Balanced Accuracy"]],
))

```

To train knn for the k value, I used cross validation. The final k that resulted the highest accuracy was `r nr_k`.

```{r fin_2,  message=FALSE, echo=FALSE, warning=FALSE}
plot_knn
```

Finally I created an ensemble with the 3 results:

```{r fin_3,  message=FALSE, echo=FALSE, warning=FALSE}
# compare results -> good results with the ensemble, but cannot beat the randomforset (rf can be already seen as an ensemble)
results2 %>% knitr::kable()
```

The ensemble could not beat the randomforest. The rf can be already seen as an ensemble, it is obtained by averaging regression trees.
To improve results further I created a bigger ensemble with more models. This helps to identify other algorithms that we could use instead of choosing in an arbitrary way. Of course the proper method would be partitioning again the training set and not use the test set when we make a choice.

```{r fin_4,  message=FALSE, echo=FALSE, warning=FALSE}
# creating an ensemble with some more models (takes time)
set.seed(1, sample.kind = "Rounding")
models <- c("glm", "lda", "naive_bayes", "svmLinear", "knn", "gamLoess", "Rborist", "qda", "rf")

fits <- lapply(models, function(model){ 
  train(default.payment.next.month ~ PAY_0+MIGRATION+PROP2, method = model, data = train2)
}) 
names(fits) <- models

#create test previsons
t <- sapply(fits,function(x){
  y_hat <- predict(x,test2, type = "raw")
})

#accuracy, speciifcity, sensitivity and BA of each model
a <- sapply(seq(1,9),function(i){
  c(confusionMatrix(data=factor(t[,i]),reference = test2$default.payment.next.month)$overall[["Accuracy"]],
  confusionMatrix(data=factor(t[,i]),reference = test2$default.payment.next.month)$byClass[["Sensitivity"]],
  confusionMatrix(data=factor(t[,i]),reference = test2$default.payment.next.month)$byClass[["Specificity"]],
  confusionMatrix(data=factor(t[,i]),reference = test2$default.payment.next.month)$byClass[["Balanced Accuracy"]])
})
colnames(a) <- models
rownames(a) <- c("Accuracy", "Sensitivity","Specificity", "Balanced Accuracy")
a %>% knitr::kable()
```

Naive Bayes, GamLoess and qda seems to fit well to our results. By majority vote I used the results of this ensemble of 8 models as well. It is comparable to our initial model using the 3 algorithms.

```{r fin_5,  message=FALSE, echo=FALSE, warning=FALSE}
#majority vote
y_hat <- factor(ifelse(rowMeans(t=="1")>0.5,"1","0"))
confusionMatrix(data=y_hat,reference = test2$default.payment.next.month)
```

 \pagebreak

# IV. Conclusion

By using only 3 variables, we were able to predict default with around 80% global accuracy. This result seems really good, but in reality banks are more intereseted in predicting default then no default. Specificity is even more important in our model then sensitivity (if the positive value is the 0 = "no default" as in our case). To take this into account, we could train our algorithms to maximize the balanced accuracy, or even give more weight (beta) to specificity when computing the F-score.



